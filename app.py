import sys
import subprocess
import os
import uuid
import time
import platform

# --- Verifica√ß√£o do Ambiente Virtual ---
def check_venv():
    """Verifica se estamos em um ambiente virtual e se √© o correto."""
    # Verifica se est√° em qualquer ambiente virtual
    in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
    if not in_venv:
        print("ERRO: Este script deve ser executado dentro do ambiente virtual.")
        print("\nPara configurar o ambiente:")
        print("1. Execute: python setup_env.py")
        print("2. Ative o ambiente:")
        if platform.system() == "Windows":
            print("   .venv\\Scripts\\activate")
        else:
            print("   source .venv/bin/activate")
        print("3. Execute novamente: python app.py")
        sys.exit(1)
    
    # Verifica se √© o ambiente virtual correto (deve estar na pasta .venv)
    expected_prefix = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.venv')
    actual_prefix = sys.prefix
    
    if platform.system() == "Windows":
        expected_prefix = expected_prefix.lower()
        actual_prefix = actual_prefix.lower()
    
    if not actual_prefix.startswith(expected_prefix):
        print("ERRO: Ambiente virtual incorreto.")
        print(f"Esperado: {expected_prefix}")
        print(f"Atual: {actual_prefix}")
        print("\nPor favor, use o ambiente virtual da pasta do projeto.")
        sys.exit(1)

# Verifica o ambiente virtual antes de qualquer outra opera√ß√£o
check_venv()

# --- Verifica√ß√£o de Ambiente e Instala√ß√£o de Depend√™ncias --- 
def check_and_install_dependencies():
    """Verifica se est√° em um venv e instala depend√™ncias do requirements.txt."""
    print("--- Verificando ambiente e depend√™ncias ---")
    
    # Verifica se o arquivo requirements.txt existe
    requirements_path = 'requirements.txt'
    if not os.path.exists(requirements_path):
        print(f"ERRO: Arquivo {requirements_path} n√£o encontrado.")
        return False

    print(f"Garantindo que as depend√™ncias em {requirements_path} est√£o instaladas...")
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', requirements_path])
        print("Depend√™ncias verificadas/instaladas com sucesso.")
        print("---------------------------------------------")
        return True
    except subprocess.CalledProcessError as e:
        print(f"ERRO: Falha ao instalar depend√™ncias: {e}")
        print("Verifique se o pip est√° funcionando e se o arquivo requirements.txt est√° correto.")
        print("---------------------------------------------")
        return False
    except FileNotFoundError:
        print("ERRO: Comando 'pip' n√£o encontrado.")
        print("Certifique-se de que Python e pip est√£o instalados e no PATH.")
        print("---------------------------------------------")
        return False

# Executa a verifica√ß√£o ANTES de tentar importar pacotes instalados
if not check_and_install_dependencies():
    sys.exit(1)

# --- Imports e L√≥gica Principal do App --- 
# S√≥ importa os pacotes DEPOIS de garantir a instala√ß√£o
import gradio as gr
from src.ollama_integration.client import chat_completion, get_available_models
from src.database.history import save_chat_message, update_feedback
from typing import List, Tuple, Dict, Any, Generator
from src.core.processing import preprocess_user_input # Importa a fun√ß√£o

# Busca a lista de modelos ANTES de definir a interface
available_models = get_available_models()
# Obt√©m o modelo padr√£o do .env para pr√©-selecionar no dropdown
default_model = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3")
# Garante que o default_model esteja na lista, caso contr√°rio usa o primeiro da lista
if default_model not in available_models and available_models:
    default_model_selected = available_models[0]
    print(f"AVISO: Modelo padr√£o '{default_model}' n√£o encontrado. Usando '{default_model_selected}' como padr√£o na UI.")
elif not available_models:
     # Caso extremo: nenhum modelo encontrado, define um fallback
     default_model_selected = "[Nenhum modelo encontrado]"
     available_models = [default_model_selected]
else:
    default_model_selected = default_model

# Fun√ß√£o principal que processa a entrada e gera a resposta
def respond(
    message: str,
    chat_history: List[Tuple[str | None, str | None]],
    selected_model: str,
    session_state: Dict[str, Any]
) -> Generator[Tuple[List[Tuple[str | None, str | None]], Dict[str, Any], str], None, None]:
    """Processa a mensagem do usu√°rio (com pr√©-processamento), chama o LLM, atualiza o hist√≥rico e mostra o tempo.

    Args:
        message: Mensagem atual do usu√°rio.
        chat_history: Hist√≥rico atual do componente Chatbot.
        selected_model: Modelo Ollama selecionado.
        session_state: Dicion√°rio de estado da sess√£o.

    Yields:
        Tupla com (hist√≥rico atualizado, estado atualizado, string de tempo).
    """
    start_time = time.time()
    time_str = ""

    # Pr√©-processa a mensagem do usu√°rio AQUI!
    processed_message = preprocess_user_input(message)
    if not processed_message:
        # Se a mensagem ficar vazia ap√≥s limpeza, n√£o faz nada
        # Apenas retorna o estado atual sem chamar LLM ou salvar
        yield chat_history, session_state, "(Mensagem vazia ap√≥s limpeza)"
        return

    # Garante/Obt√©m session_id e inicializa last_message_id se necess√°rio
    if "session_id" not in session_state:
        session_state["session_id"] = str(uuid.uuid4())
        session_state["last_db_message_id"] = None # Inicializa ID
    session_id = session_state["session_id"]

    # Formata mensagens para a API
    messages = []
    for user_msg, assistant_msg in chat_history:
        if user_msg:
             messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
             messages.append({"role": "assistant", "content": assistant_msg})
    # Adiciona a mensagem PROCESSADA √† lista para a API
    messages.append({"role": "user", "content": processed_message})

    # Zera o ID da √∫ltima mensagem antes de gerar nova resposta
    session_state["last_db_message_id"] = None

    # Adiciona a mensagem PROCESSADA ao hist√≥rico da UI
    # (Mostra ao usu√°rio a mensagem como ele ver√° ap√≥s a limpeza)
    chat_history.append((processed_message, None))
    yield chat_history, session_state, time_str

    # Chama o LLM com a mensagem processada (impl√≠cito, pois est√° em `messages`)
    response_generator = chat_completion(messages=messages, model=selected_model, stream=True)
    full_response = ""

    try:
        if response_generator:
            for chunk in response_generator:
                full_response += chunk
                # Atualiza a √∫ltima mensagem usando a processed_message como chave
                chat_history[-1] = (processed_message, full_response)
                yield chat_history, session_state, time_str
        else:
            full_response = "Desculpe, ocorreu um erro ao contatar o modelo."
            chat_history[-1] = (processed_message, full_response)
            yield chat_history, session_state, time_str
    finally:
        end_time = time.time()
        duration = end_time - start_time
        time_str = f"Tempo de resposta: {duration:.2f}s"
        print(time_str)

        # Salva no banco de dados e guarda o ID
        saved_id = None
        if full_response and full_response != "Desculpe, ocorreu um erro ao contatar o modelo.":
             saved_id = save_chat_message(user_message=processed_message, assistant_message=full_response, session_id=session_id)
        
        # Armazena o ID da mensagem salva no estado da sess√£o
        session_state["last_db_message_id"] = saved_id

    yield chat_history, session_state, time_str

# --- Nova Fun√ß√£o para Lidar com Feedback --- 
def handle_feedback(feedback_type: str, session_state: Dict[str, Any]) -> None:
    """Atualiza o feedback no banco de dados para a √∫ltima mensagem salva."""
    last_message_id = session_state.get("last_db_message_id")
    feedback_value = 1 if feedback_type == "üëç" else -1 if feedback_type == "üëé" else 0

    if last_message_id is not None and feedback_value != 0:
        print(f"Registrando feedback {feedback_type} para a mensagem ID: {last_message_id}")
        update_feedback(message_id=last_message_id, feedback_value=feedback_value)
        # Poderia adicionar um gr.Info ou gr.Warning aqui para confirmar ao usu√°rio
        # Ex: gr.Info(f"Feedback {feedback_type} registrado!") - mas requer retorno
    elif feedback_value == 0:
        print("Tipo de feedback inv√°lido recebido.")
    else:
        print("Nenhuma mensagem anterior encontrada nesta sess√£o para registrar feedback.")

# --- Defini√ß√£o da Interface com gr.Blocks --- 
with gr.Blocks(theme=gr.themes.Default(primary_hue="blue", secondary_hue="neutral")) as demo:
    # Estado da sess√£o (para session_id)
    session_state = gr.State({})

    gr.Markdown("# Meu Chatbot com Ollama")

    # Seletor de Modelo (acima do chat)
    model_selector = gr.Dropdown(
        choices=available_models,
        value=default_model_selected,
        label="Escolha o Modelo Ollama",
        interactive=True
    )

    # √Årea do Chat
    chatbot = gr.Chatbot(
        label="Chat",
        bubble_full_width=False,
        height=500 # Ajuste a altura conforme necess√°rio
    )

    # Adiciona componente para exibir o tempo
    time_output = gr.Markdown("")

    # Adiciona linha para bot√µes de feedback
    with gr.Row() as feedback_row:
        feedback_label = gr.Markdown("Feedback da √∫ltima resposta:", visible=True) # Ou False inicialmente
        thumb_up_btn = gr.Button("üëç")
        thumb_down_btn = gr.Button("üëé")

    # √Årea de Input
    with gr.Row():
        msg_input = gr.Textbox(
            scale=4,
            show_label=False,
            placeholder="Digite sua mensagem aqui...",
            container=False,
        )
        send_button = gr.Button("Enviar", scale=1)

    # A√ß√µes de Limpeza (Opcional)
    # clear_button = gr.ClearButton([msg_input, chatbot])

    # --- Conex√£o dos Eventos --- 

    # Fun√ß√£o para limpar APENAS o input ap√≥s envio
    def clear_message_input_only():
        return ""

    # Quando o usu√°rio pressiona Enter no Textbox (msg_input)
    msg_input.submit(
        respond, # Fun√ß√£o a ser chamada
        [msg_input, chatbot, model_selector, session_state], # Inputs da fun√ß√£o
        # Adiciona time_output aos outputs
        [chatbot, session_state, time_output], # Outputs da fun√ß√£o (atualiza o chatbot e o state)
        queue=True # Permite processamento em fila
    # Limpa APENAS msg_input ap√≥s a resposta
    ).then(clear_message_input_only, [], [msg_input])

    # Quando o usu√°rio clica no bot√£o Enviar
    send_button.click(
        respond,
        [msg_input, chatbot, model_selector, session_state],
        [chatbot, session_state, time_output],
        queue=True
    # Limpa APENAS msg_input ap√≥s a resposta
    ).then(clear_message_input_only, [], [msg_input])

    # Conecta bot√µes de feedback √† fun√ß√£o handle_feedback
    thumb_up_btn.click(
        handle_feedback,
        inputs=[gr.Textbox("üëç", visible=False), session_state], # Passa tipo de feedback escondido
        outputs=None # N√£o atualiza a UI diretamente, s√≥ o BD
    )
    thumb_down_btn.click(
        handle_feedback,
        inputs=[gr.Textbox("üëé", visible=False), session_state],
        outputs=None
    )

# Lan√ßa a aplica√ß√£o web
if __name__ == "__main__":
    demo.launch(share=False) 